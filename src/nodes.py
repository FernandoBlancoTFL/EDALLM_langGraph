from config import df, llm
from tools import tools
from utils import build_code_prompt, run_python_with_df
from typing import List
from langchain.agents import Tool
from state import AgentState

def get_tools_summary(tools: List[Tool]) -> str:
    """Devuelve un resumen con nombre y descripci√≥n de cada tool."""
    return "\n".join([f"- {t.name}: {t.description}" for t in tools])

def node_clasificar(state: AgentState):
    """El LLM decide qu√© acci√≥n tomar con contexto mejorado"""
    
    # Obtener informaci√≥n del DataFrame si no existe
    if not state.get("df_info"):
        state["df_info"] = {
            "columns": list(df.columns),
            "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
            "shape": df.shape,
            "sample": df.head(2).to_dict()
        }
    
    # Contexto de iteraciones previas
    iteration_context = ""
    if state["iteration_count"] > 0:
        iteration_context = f"\nEsta es la iteraci√≥n #{state['iteration_count'] + 1}. Intentos previos han fallado."
        if state["execution_history"]:
            last_error = state["execution_history"][-1].get("error", "")
            iteration_context += f"\n√öltimo error: {last_error}"

    tools_summary = get_tools_summary(tools)

    prompt = f"""
        Eres un asistente de an√°lisis de datos experto. Analiza esta consulta y decide la mejor acci√≥n.

        CONSULTA: {state['query']}
        DATAFRAME INFO: Columnas = {state['df_info']['columns']}, Shape = {state['df_info']['shape']}
        {iteration_context}

        HERRAMIENTAS DISPONIBLES:
        {tools_summary}

        DECISI√ìN:
        Analiza la consulta y selecciona la herramienta m√°s adecuada. 
        Si ninguna herramienta especializada es suficiente, usa Python_Interpreter.

        Formato de salida:
        Thought: <an√°lisis detallado de la consulta y estrategia>
        Action: <nombre exacto de la herramienta elegida>
    """

    response = llm.invoke(prompt).content.strip()

    # Extraer thought y action
    thought, action = "", "Python_Interpreter"
    for line in response.splitlines():
        if line.lower().startswith("thought:"):
            thought = line.split(":", 1)[1].strip()
        elif line.lower().startswith("action:"):
            action = line.split(":", 1)[1].strip()

    state["thought"] = thought
    state["action"] = action
    state["history"].append(f"Iteraci√≥n {state['iteration_count']} - Clasificar ‚Üí {thought[:100]}...")

    print(f"\nüß† Iteraci√≥n {state['iteration_count']} - Thought: {thought}")
    print(f"‚û°Ô∏è Action: {action}")

    return state

def node_ejecutar_python(state: AgentState):
    """Ejecuta c√≥digo Python con manejo robusto de errores y contexto"""
    
    print(f"‚öôÔ∏è Ejecutando Python - Intento {state['iteration_count'] + 1}")
    
    # Generar c√≥digo con contexto completo
    code_prompt = build_code_prompt(
        state["query"], 
        state["execution_history"], 
        state["df_info"]
    )
    
    # Generar c√≥digo
    python_code = llm.invoke(code_prompt).content.strip()
    
    # Limpiar markdown
    if python_code.startswith("```"):
        python_code = python_code.strip("`")
        if python_code.lower().startswith("python"):
            python_code = python_code[len("python"):].strip()
        python_code = python_code.replace("```", "").strip()

    print(f"\nüîç C√≥digo generado:\n{python_code}")
    
    # Ejecutar c√≥digo
    execution_result = run_python_with_df(python_code)
    
    # Crear registro de ejecuci√≥n
    execution_record = {
        "iteration": state["iteration_count"],
        "code": python_code,
        "success": execution_result["success"],
        "result": execution_result["result"],
        "error": execution_result["error"],
        "error_type": execution_result["error_type"]
    }
    
    # Actualizar historial
    state["execution_history"].append(execution_record)
    state["result"] = execution_result["result"]
    state["success"] = execution_result["success"]
    
    if execution_result["success"]:
        print(f"‚úÖ √âxito: {execution_result['result']}")
    else:
        print(f"‚ùå Error: {execution_result['error']}")
        state["final_error"] = execution_result["error"]
    
    state["history"].append(f"Ejecutar Python ‚Üí {'√âxito' if execution_result['success'] else 'Error: ' + str(execution_result['error'])}")
    
    return state

def node_validar_y_decidir(state: AgentState):
    """Valida el resultado y decide si continuar iterando"""
    
    state["iteration_count"] += 1
    success = state.get("success", False)
    max_iterations = state.get("max_iterations", 3)
    
    print(f"\nüîç Validaci√≥n - Iteraci√≥n {state['iteration_count']}")
    print(f"   √âxito: {success}")
    print(f"   Iteraciones restantes: {max_iterations - state['iteration_count']}")
    
    # Decidir pr√≥xima acci√≥n
    if success:
        state["next_node"] = "responder"
        print("   ‚û°Ô∏è Decisi√≥n: Proceder a responder (√©xito)")
    elif state["iteration_count"] >= max_iterations:
        state["next_node"] = "responder"
        print("   ‚û°Ô∏è Decisi√≥n: Proceder a responder (m√°ximo de iteraciones alcanzado)")
    else:
        state["next_node"] = "clasificar"
        print("   ‚û°Ô∏è Decisi√≥n: Nueva iteraci√≥n")
    
    state["history"].append(f"Validar ‚Üí Iteraci√≥n {state['iteration_count']}, √âxito: {success}, Pr√≥ximo: {state['next_node']}")
    
    return state

def node_responder(state: AgentState):
    """Genera la respuesta final basada en todo el contexto"""
    
    success = state.get("success", False)
    
    if success:
        prompt = f"""
Pregunta del usuario: {state['query']}
Resultado obtenido: {state['result']}
N√∫mero de iteraciones necesarias: {state['iteration_count']}

Genera una respuesta clara y amigable en espa√±ol explicando qu√© se logr√≥.
"""
    else:
        # Analizar todos los errores para dar una respuesta informativa
        errors_summary = []
        for record in state["execution_history"]:
            if not record["success"]:
                errors_summary.append(f"- {record['error_type']}: {record['error']}")
        
        prompt = f"""
Pregunta del usuario: {state['query']}
Despu√©s de {state['iteration_count']} iteraciones, no se pudo completar la tarea.

Errores encontrados:
{chr(10).join(errors_summary)}

Genera una respuesta emp√°tica en espa√±ol explicando:
1. Que se intent√≥ resolver la consulta m√∫ltiples veces
2. Los principales problemas encontrados (en t√©rminos simples)
3. Sugerencias para el usuario (ej: verificar formato de datos, columnas, etc.)
"""

    respuesta = llm.invoke(prompt).content
    print(f"\nü§ñ Respuesta Final:\n{respuesta}")
    
    # Log final
    state["history"].append(f"Responder ‚Üí Finalizado con {'√©xito' if success else 'error'}")
    
    return state

def route_after_validation(state: AgentState):
    """Determina la siguiente ruta basada en la validaci√≥n"""
    success = state.get("success", False)
    iteration_count = state.get("iteration_count", 0)
    max_iterations = state.get("max_iterations", 3)
    
    print(f"\nüîß DEBUG route_after_validation:")
    print(f"   Success: {success}")
    print(f"   Iteration: {iteration_count}")
    print(f"   Max iterations: {max_iterations}")
    print(f"   Next node from state: {state.get('next_node', 'N/A')}")
    
    if success:
        print("   ‚Üí Routing to: responder (success)")
        return "responder"
    elif iteration_count >= max_iterations:
        print("   ‚Üí Routing to: responder (max iterations reached)")
        return "responder"
    else:
        print("   ‚Üí Routing to: clasificar (continue iteration)")
        return "clasificar"

# Copia aqu√≠ tus funciones node_clasificar, node_ejecutar_python, node_validar_y_decidir,
# node_responder y route_after_validation, ajustando imports para usar utils/config.
